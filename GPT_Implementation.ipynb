{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerahmed12345elhussien/GPT_Implementation/blob/main/GPT_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "I3__XW7Fz2o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import Tensor\n",
        "from typing import *\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "w2dstREaz3NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full implementation"
      ],
      "metadata": {
        "id": "Dt0mww9Ag8qU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters"
      ],
      "metadata": {
        "id": "drjpAb7EhEq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The ratio of traing to validation datasets\n",
        "split_portion=0.9\n",
        "#The number of independent sequences that can be processed in parallel\n",
        "batch_size=64\n",
        "#The size of the context\n",
        "block_size=256\n",
        "num_embed=384\n",
        "#The number of head in the self-attention\n",
        "num_head=6\n",
        "#The number of blocks of the decoder\n",
        "num_layer=6\n",
        "#The ratio of weights to be set to zero. For overfitting\n",
        "dropout_value=0.2\n",
        "eval_iters = 500\n",
        "eval_interval = 1000\n",
        "learning_rate=3e-4\n",
        "num_epochs=5000"
      ],
      "metadata": {
        "id": "RwWQLIZug_08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions + Classes"
      ],
      "metadata": {
        "id": "N_EyUaV5hJPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the random seeds\n",
        "torch.manual_seed(40)\n",
        "\n",
        "#Downloading the data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "#Reading the data\n",
        "with open('input.txt','r',encoding='utf-8') as file:\n",
        "  content=file.read()\n",
        "\n",
        "#Determine the unique characters in input.txt\n",
        "unq_char= []\n",
        "for char in content:\n",
        "  if char not in unq_char:\n",
        "    unq_char.append(char)\n",
        "unq_char=sorted(unq_char)\n",
        "#The vocabulary size.\n",
        "voc_size=len(unq_char)\n",
        "\n",
        "#Mapping the data from characters to inegers.\n",
        "char_to_int=defaultdict(lambda: 0)\n",
        "int_to_char=defaultdict(lambda: '')\n",
        "for idx, char in enumerate(unq_char):\n",
        "  char_to_int[char]=idx\n",
        "  int_to_char[idx]=char\n",
        "\n",
        "# The encoder: takes a string, and output a list of integers.\n",
        "encoder=lambda inp: [char_to_int[element] for element in inp]\n",
        "#The decoder: takes a list of integers, and ouput a string.\n",
        "decoder= lambda inp: ''.join([ int_to_char[numb] for numb in inp])\n",
        "#Encode the all data and store it in a tensor\n",
        "data_enc=torch.tensor(encoder(content)).long()\n",
        "\n",
        "#Split the data to train, validation\n",
        "split_size=int(split_portion*len(data_enc))\n",
        "train_data=data_enc[:split_size]\n",
        "valid_data=data_enc[split_size:]\n",
        "\n",
        "#Data loader\n",
        "def gen_batch(inpt: str)->tuple:\n",
        "  \"\"\"\n",
        "  gen_batch: accepts a string train or valid\n",
        "  It returns: a tuple of two tensors of context x, and target y.\n",
        "  \"\"\"\n",
        "  req_data=train_data if inpt=='train' else valid_data\n",
        "  idx=torch.randint(len(req_data)-block_size,size=(batch_size,))\n",
        "  x= torch.vstack([req_data[i:i+block_size] for i in idx])\n",
        "  y= torch.vstack([req_data[i+1:i+block_size+1] for i in idx])\n",
        "  return x.to(device),y.to(device)\n",
        "\n",
        "#Estimate loss function\n",
        "@torch.no_grad()\n",
        "def estimate_loss_fun(model)->dict:\n",
        "  #Initalize our output dictionary\n",
        "  output={}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses=torch.zeros(eval_iters)\n",
        "    for num in range(eval_iters):\n",
        "      context,target=gen_batch(split)\n",
        "      logit,loss=model(context,target)\n",
        "      losses[num]=loss.item()\n",
        "    output[split]=losses.mean()\n",
        "  model.train()\n",
        "  return output\n",
        "\n",
        "#Single Head Implementation\n",
        "class Head(nn.Module):\n",
        "  \"\"\"Single head of self-attention\"\"\"\n",
        "  #Class constructor\n",
        "  def __init__(self,head_size:int)->None:\n",
        "    super().__init__()\n",
        "    #Linear projections\n",
        "    self.key=nn.Linear(num_embed,head_size,bias=False)\n",
        "    self.query=nn.Linear(num_embed,head_size,bias=False)\n",
        "    self.value=nn.Linear(num_embed,head_size,bias=False)\n",
        "    self.register_buffer('low_tri',torch.tril(torch.ones(block_size,block_size)))\n",
        "    self.dropout=nn.Dropout(dropout_value)\n",
        "\n",
        "  def forward(self,tok_emb:Tensor)->Tensor:\n",
        "    Batch_size, Block_size, num_embed=tok_emb.shape\n",
        "    #k,q, and v of size (Batch_size, Block_size, num_embed)\n",
        "    k=self.key(tok_emb)\n",
        "    q=self.query(tok_emb)\n",
        "    v=self.value(tok_emb)\n",
        "    #Calculate attention scores\n",
        "    weight=q@k.transpose(-2,-1)* num_embed**-0.5 #(Batch_size, block , num_embed) @ (Batch_size, num_embed, block) ----> (Batch_size, num_embed, num_embed)\n",
        "    weight=weight.masked_fill(self.low_tri[:Block_size,:Block_size]==0,float('-inf')) #Of size (Batch_size,num_embed,num_embed)\n",
        "    weight=F.softmax(weight,dim=-1) #Of size (Batch_size,num_embed)\n",
        "    output=weight@v # (Batch_size,num_embed) @ (Batch_size, Block_size, num_embed) ----> (Batch_size, Block_size, num_embed)\n",
        "    return output\n",
        "\n",
        "#Multiple Head class Implementation\n",
        "class Multi_Head(nn.Module):\n",
        "  \"\"\"Multi-head self-attention implementation \"\"\"\n",
        "  #class constructor\n",
        "  def __init__(self,num_head:int, head_size:int)->None:\n",
        "    super().__init__()\n",
        "    self.heads= nn.ModuleList([Head(head_size) for i in range(num_head)])\n",
        "    self.lin_proj=nn.Linear(num_embed,num_embed)\n",
        "    self.dropout=nn.Dropout(dropout_value)\n",
        "\n",
        "  def forward(self,tok_emb:Tensor)->Tensor:\n",
        "    output=torch.cat([head(tok_emb) for head in self.heads],dim=-1)\n",
        "    output=self.dropout(self.lin_proj(output))\n",
        "    return output\n",
        "\n",
        "#Feed forward Networks\n",
        "class FF_Networ(nn.Module):\n",
        "  \"\"\"a simple NN with single linear projection and ReLU\"\"\"\n",
        "  #Class constructor\n",
        "  def __init__(self,num_embed:int)->None:\n",
        "    super().__init__()\n",
        "    self.network=nn.Sequential(\n",
        "        nn.Linear(num_embed,4*num_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*num_embed,num_embed),\n",
        "        nn.Dropout(dropout_value)\n",
        "    )\n",
        "\n",
        "  def forward(self,valu:Tensor)->Tensor:\n",
        "    return self.network(valu)\n",
        "\n",
        "#Transformer blocks\n",
        "class Transf_Block(nn.Module):\n",
        "  #Class constructor\n",
        "  def __init__(self,num_embed:int,num_head:int)->None:\n",
        "    super().__init__()\n",
        "    head_size=num_embed//num_head\n",
        "    self.mul_head=Multi_Head(num_head,head_size)\n",
        "    self.ff_net=FF_Networ(num_embed)\n",
        "    self.ln1=nn.LayerNorm(num_embed)\n",
        "    self.ln2=nn.LayerNorm(num_embed)\n",
        "\n",
        "  def forward(self, x:Tensor)->Tensor:\n",
        "    #Add residual connection\n",
        "    x=x+self.mul_head(self.ln1(x))\n",
        "    #Add residual connection\n",
        "    x=x+self.ff_net(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "#GPT Implementation class\n",
        "class GPT_Imp(nn.Module):\n",
        "  #Class constructor\n",
        "  def __init__(self)->None:\n",
        "    super().__init__()\n",
        "    #a lookup table of size (voc_size,num_embed)\n",
        "    self.token_emb=nn.Embedding(voc_size,num_embed)\n",
        "    self.position_emb=nn.Embedding(block_size,num_embed)\n",
        "    self.blocks=nn.Sequential(*[Transf_Block(num_embed,num_head) for i in range(num_layer)])\n",
        "    self.f_ln=nn.LayerNorm(num_embed)\n",
        "    self.lin_head=nn.Linear(num_embed,voc_size)\n",
        "\n",
        "  def forward(self,context:Tensor, target:Tensor=None)->tuple:\n",
        "    #context and target are of size (Batch_size, Block_size)\n",
        "    Batch_size, Block_size=context.shape\n",
        "    tok_emb=self.token_emb(context) #Of size (Batch_size, Block_size, num_embed)\n",
        "    pos_emb=self.position_emb(torch.arange(Block_size,device=device)) #Of size (Block_size,num_embed)\n",
        "    sum_tok=tok_emb+pos_emb #Of size (Batch_size, Block_size, num_embed)\n",
        "    sum_tok=self.blocks(sum_tok)\n",
        "    sum_tok=self.f_ln(sum_tok)\n",
        "    logit=self.lin_head(sum_tok) #Of size (Batch_size, Block_size, voc_size)\n",
        "    if target is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      Batch_size, Block_size, voc_size=logit.shape\n",
        "      #This reshape step is due to cross_entropy requirement that C:voc_size, should be the second dimension.\n",
        "      logit=logit.reshape(Batch_size*Block_size, voc_size)\n",
        "      target= target.reshape(Batch_size*Block_size)\n",
        "      loss=F.cross_entropy(logit,target)\n",
        "    return logit, loss\n",
        "\n",
        "  def generate(self, context:Tensor, max_new_tok:Tensor)->Tensor:\n",
        "    #The job of this function is to generate next text for the given number of max_new_tok\n",
        "    for i in range(max_new_tok):\n",
        "      #Consider only last part of context of size: block_size\n",
        "      context_cond=context[:,-block_size:]\n",
        "      #Make prediction\n",
        "      logit,loss =self(context_cond)\n",
        "      #For the case of bigram, focus on the last element of the block_size. So, we have shape (Batch_size, voc_size)\n",
        "      logit=logit[:,-1,:]\n",
        "      #Apply softmax\n",
        "      prob=F.softmax(logit,dim=-1)\n",
        "      # Take a sample. We get shape (Batch_size,1)\n",
        "      context_next= torch.multinomial(prob, num_samples=1)\n",
        "      #Append context_next to context. The new shape is (Batch_size,Block_size+1)\n",
        "      context=torch.hstack((context,context_next))\n",
        "    return context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmP_ExQ9g_s2",
        "outputId": "41b1333b-728f-4c0d-ecb0-8c9450dc935c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-25 20:50:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-06-25 20:50:53 (158 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_ob= GPT_Imp().to(device)\n",
        "#Optimizer\n",
        "optimizer = torch.optim.AdamW(gpt_ob.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "bOSlhgfNL3iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "for iter in range(num_epochs):\n",
        "\n",
        "  #Evaluate the loss on the train and validation datasets\n",
        "  if iter % eval_interval == 0 or iter == num_epochs - 1:\n",
        "      loss = estimate_loss_fun(gpt_ob)\n",
        "      print(f\"step {iter}: train loss {loss['train']:.4f}, val loss {loss['val']:.4f}\")\n",
        "  #Sample from the data with the given batch_size\n",
        "  context,target=gen_batch('train')\n",
        "  #Forward pass to get logit and loss\n",
        "  logit,loss=gpt_ob(context,target)\n",
        "  # Clear gradients w.r.t. parameters\n",
        "  optimizer.zero_grad()\n",
        "  # Getting gradients w.r.t. parameters\n",
        "  loss.backward()\n",
        "  # Updates parameters:\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgYV2GtFM4a_",
        "outputId": "1fcaa939-9d1c-4375-a296-5d3533ba0d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3686, val loss 4.3661\n",
            "step 1000: train loss 1.5951, val loss 1.7802\n",
            "step 2000: train loss 1.3357, val loss 1.5834\n",
            "step 3000: train loss 1.2107, val loss 1.5167\n",
            "step 4000: train loss 1.1203, val loss 1.5143\n",
            "step 4999: train loss 1.0403, val loss 1.5389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate new words\n",
        "print(decoder(gpt_ob.generate(torch.zeros((1,1),device=device).long(),3000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOAKeoYHMT-b",
        "outputId": "a23246a4-8412-410d-dabb-970a4356e915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shepherd by the goods to no need of a king?\n",
            "\n",
            "BRUTUS:\n",
            "Neither neighbours, his of my vow; but leave\n",
            "to meet only.\n",
            "\n",
            "SICINIUS:\n",
            "Ineither, sir? us that the true matter was\n",
            "two beastles of\n",
            "The triumphant and and his shamed thrusts piece\n",
            "To thy gainsays, we'll raze a modester of hours!\n",
            "\n",
            "Third Servingman:\n",
            "In all, you and the last which wrought of devotion\n",
            "That I would live,\n",
            "This were from of hand, somethicable, that I ould:\n",
            "In Somer Clifford you and Mowbray, hence,\n",
            "Even from me; thou hast have left the heart of it\n",
            "Read mile may me. I, there's so ink to the\n",
            "farmer than stand her honesty 'Work both taken; and if\n",
            "Dare leanness princess, and that close of him\n",
            "Which manner came complaints with us.\n",
            "\n",
            "LORD STERSTEY:\n",
            "I say, if you unhadame,\n",
            "Your cannot broud him to back blindards blowled:\n",
            "Starr'd him, you cousin his segrance years,\n",
            "And, and s, you play I more than you might shall\n",
            "To redress that do far marry and all the guilder,\n",
            "But low'd those and than you are much to-day,\n",
            "To badgare the provoke man. Look, and Tress,\n",
            "Forthwith from all this heads of bloody is,\n",
            "That my physick itself more over\n",
            "Than husband me thy trembland and faults?\n",
            "\n",
            "Therd Messenger:\n",
            "The devoury father's land's rod proffer shed;\n",
            "Thus for this new better salues as my woman;\n",
            "And then he highld is press'd his friends,\n",
            "And stark from the closes whence which did shake flies,\n",
            "Up sovereablessed new that doubt use!\n",
            "Have done, on thy vow made thy sons nothing\n",
            "That comforted the dries of my son\n",
            "Tale our golden, whose harp grivate in thee\n",
            "Thy haced is dead weeping and and thy head!\n",
            "Good make them, I, am for gentleman,\n",
            "At happy myself argree on his maw'd,\n",
            "We trust this mother fiercing embraces,\n",
            "And this so was well to stain dishemed\n",
            "With his vhiolen-like days rhein again.\n",
            "\n",
            "EDWARD:\n",
            "And shame, bloods muster to-night to age,\n",
            "I'll prett you might to be a glossing king?\n",
            "Now, let them pilgrim they person,\n",
            "Thy might had unto askind the business;\n",
            "And leaden the contrary of the boar\n",
            "Can assignance: for how means my intent\n",
            "As may clamp my back and noble grave\n",
            "Is something now remimedy\n",
            "To should bear their brother here and their:\n",
            "O, Montsua: they comes may see too so,\n",
            "As a dreemen in the lasws, and safe?\n",
            "\n",
            "MONTAGUE:\n",
            "Marry, then, shalt be. Go, canst you not.\n",
            "\n",
            "GREMIO:\n",
            "Thou'rany should dram. A mercy case comes to about thy\n",
            "hellow; and is not to them? Nay, come I kneel.\n",
            "\n",
            "GRUMIO:\n",
            "It that husband it be Pompey and so\n",
            "love--naving Mercutio's flesh, and rough against eleves,\n",
            "For given where then which'd notes they not do the fortune's\n",
            "Thus forth their divine. O false foul of day\n",
            "Redempenive the grazine of your Rume,\n",
            "That he great denied lived a loving and\n",
            "Toward Tybalt Jove he, here pardone, with with the;\n",
            "And every tongue the heart that the common muarch\n",
            "This accuser of my master'd.\n",
            "\n",
            "SLY:urp were whence I recould here my lips,\n",
            "As well make you within you fixmans; and how\n",
            "Should not carriepe all this poor and treasure\n",
            "With to service\n",
            "In shepherds lenders: then I'll forth them to beg;\n",
            "And stain, but, as good trims, die:\n",
            "Sly, con\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltgk9E-jWUF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References\n",
        "We implement this code following: https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy\n"
      ],
      "metadata": {
        "id": "taz33eGsOggw"
      }
    }
  ]
}